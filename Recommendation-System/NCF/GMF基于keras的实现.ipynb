{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-30T02:51:30.496857Z",
     "start_time": "2020-08-30T02:51:30.490367Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, path):\n",
    "        self.trainMatrix = self.load_rating_file_as_matrix(path + \".train.rating\")\n",
    "        self.testRatings = self.load_rating_file_as_list(path + \".test.rating\")\n",
    "        self.testNegatives = self.load_negative_file(path + \".test.negative\")\n",
    "        assert len(self.testRatings) == len(self.testNegatives)\n",
    "        \n",
    "        self.num_users, self.num_items = self.trainMatrix.shape\n",
    "\n",
    "    def load_rating_file_as_list(self, filename):\n",
    "        df = pd.read_csv(filename, sep=\"\\t\")\n",
    "        ratingList = list(zip(df.userid.tolist(), df.itemid.tolist()))\n",
    "        return ratingList\n",
    "    \n",
    "    def load_negative_file(self, filename):\n",
    "        df = pd.read_csv(filename, sep=\"\\t\")\n",
    "        negativeList = df.iloc[:, 1:].values.tolist()\n",
    "        return negativeList\n",
    "    \n",
    "    def load_rating_file_as_matrix(self, filename):\n",
    "        df = pd.read_csv(filename, sep=\"\\t\")\n",
    "        num_users = df.userid.max()\n",
    "        num_items = df.itemid.max()\n",
    "        mat = sp.dok_matrix((num_users + 1, num_items + 1), dtype=np.float32)\n",
    "        interactions = df[['userid', 'itemid']].values.tolist()\n",
    "        # [(0, 2969), (0, 1178), (0, 1574), (0, 957)]\n",
    "        for user, item in interactions:\n",
    "            mat[user, item] = 1.\n",
    "        # [((0, 2969), 1.0), ((0, 1178), 1.0), ((0, 1574), 1.0), ((0, 957), 1.0)]\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-30T02:51:32.363445Z",
     "start_time": "2020-08-30T02:51:32.356689Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import heapq\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_model(model, testRatings, testNegatives, topK):\n",
    "\n",
    "    global _model\n",
    "    global _testRatings\n",
    "    global _testNegatives\n",
    "    global _topK\n",
    "\n",
    "    _model = model\n",
    "    _testRatings = testRatings\n",
    "    _testNegatives = testNegatives\n",
    "    _topK = topK\n",
    "\n",
    "    hits, ndcgs = [],[]\n",
    "    for idx in range(len(_testRatings)):\n",
    "        (hr,ndcg) = eval_one_rating(idx)\n",
    "        hits.append(hr)\n",
    "        ndcgs.append(ndcg)\n",
    "    return (hits, ndcgs)\n",
    "\n",
    "\n",
    "def eval_one_rating(idx):\n",
    "    rating = _testRatings[idx]\n",
    "    items = _testNegatives[idx]\n",
    "    u = rating[0]\n",
    "    gtItem = rating[1]\n",
    "    items.append(gtItem)\n",
    "    map_item_score = {}\n",
    "    users = np.full(len(items), u, dtype = 'int32')\n",
    "    predictions = _model.predict([users, np.array(items)],\n",
    "                                 batch_size=100, verbose=0)\n",
    "\n",
    "    for i in range(len(items)):\n",
    "        item = items[i]\n",
    "        map_item_score[item] = predictions[i]\n",
    "    items.pop()\n",
    "\n",
    "    ranklist = heapq.nlargest(_topK, map_item_score, key=map_item_score.get)\n",
    "    hr = getHitRatio(ranklist, gtItem)\n",
    "    ndcg = getNDCG(ranklist, gtItem)\n",
    "    return (hr, ndcg)\n",
    "\n",
    "\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "def get_train_instances(train, n_items, n_neg, testNegatives):\n",
    "    user, item, labels = [],[],[]\n",
    "    n_users = train.shape[0]\n",
    "    for (u, i) in train.keys():\n",
    "        # positive instance\n",
    "        user.append(u)\n",
    "        item.append(i)\n",
    "        labels.append(1)\n",
    "        # negative instances: we also need to make sure they are not in the\n",
    "        # test dataset\n",
    "        for t in range(n_neg):\n",
    "            j = np.random.randint(n_items)\n",
    "            while ((u, j) in train.keys()) or (j in testNegatives[u]):\n",
    "                j = np.random.randint(n_items)\n",
    "            user.append(u)\n",
    "            item.append(j)\n",
    "            labels.append(0)\n",
    "    return np.array(user), np.array(item), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-30T03:20:23.077214Z",
     "start_time": "2020-08-30T02:51:36.958958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3814: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3075: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:977: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:964: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2503: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:184: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:193: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jiang/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:200: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "Iteration 1: 55.03s, HR = 0.5674, NDCG = 0.3135, loss = 0.3312, validated in 4.73s\n",
      "Iteration 2: 54.67s, HR = 0.5826, NDCG = 0.3248, loss = 0.3019, validated in 4.67s\n",
      "Iteration 3: 51.98s, HR = 0.5750, NDCG = 0.3234, loss = 0.2989, validated in 4.90s\n",
      "Iteration 4: 53.73s, HR = 0.6051, NDCG = 0.3381, loss = 0.2930, validated in 5.02s\n",
      "Iteration 5: 56.02s, HR = 0.6147, NDCG = 0.3495, loss = 0.2856, validated in 5.88s\n",
      "Iteration 6: 82.54s, HR = 0.6286, NDCG = 0.3569, loss = 0.2812, validated in 8.41s\n",
      "Iteration 7: 71.14s, HR = 0.6321, NDCG = 0.3582, loss = 0.2803, validated in 11.14s\n",
      "Iteration 8: 70.75s, HR = 0.6174, NDCG = 0.3510, loss = 0.2801, validated in 10.13s\n",
      "Iteration 9: 77.31s, HR = 0.6371, NDCG = 0.3621, loss = 0.2802, validated in 12.30s\n",
      "Iteration 10: 79.50s, HR = 0.6318, NDCG = 0.3586, loss = 0.2807, validated in 11.20s\n",
      "Iteration 11: 85.42s, HR = 0.6315, NDCG = 0.3579, loss = 0.2810, validated in 5.77s\n",
      "Iteration 12: 89.96s, HR = 0.6270, NDCG = 0.3530, loss = 0.2815, validated in 5.87s\n",
      "Iteration 13: 90.42s, HR = 0.6356, NDCG = 0.3588, loss = 0.2819, validated in 6.15s\n",
      "Iteration 14: 99.35s, HR = 0.6308, NDCG = 0.3601, loss = 0.2824, validated in 7.71s\n",
      "Iteration 15: 82.25s, HR = 0.6300, NDCG = 0.3580, loss = 0.2828, validated in 11.18s\n",
      "Iteration 16: 81.02s, HR = 0.6142, NDCG = 0.3448, loss = 0.2832, validated in 12.96s\n",
      "Iteration 17: 79.85s, HR = 0.6174, NDCG = 0.3498, loss = 0.2841, validated in 11.10s\n",
      "Iteration 18: 76.77s, HR = 0.6144, NDCG = 0.3481, loss = 0.2843, validated in 11.51s\n",
      "Iteration 19: 89.96s, HR = 0.6222, NDCG = 0.3477, loss = 0.2849, validated in 9.80s\n",
      "Iteration 20: 116.28s, HR = 0.6084, NDCG = 0.3404, loss = 0.2850, validated in 8.77s\n",
      "End. Best Iteration 9:  HR = 0.6371, NDCG = 0.3621. \n",
      "The best GMF model is saved to models/keras_GMF_bs_256_reg_00_lr_001_n_emb_8.h5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import heapq\n",
    "import keras\n",
    "import multiprocessing as mp\n",
    "from keras import initializers\n",
    "from keras.models import Model, load_model, save_model\n",
    "from keras.layers import Dense, Embedding, Input, merge, Flatten, multiply\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from time import time\n",
    "\n",
    "def GMF(n_users, n_items, n_emb, reg):\n",
    "    user = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "    item = Input(shape=(1,), dtype='int32', name='item_input')\n",
    "\n",
    "    # user and item embeddings\n",
    "    # [?, 1, 8]\n",
    "    MF_Embedding_User = Embedding(\n",
    "        input_dim=n_users,\n",
    "        output_dim=n_emb,\n",
    "        name='user_embedding',\n",
    "        embeddings_initializer='normal',\n",
    "        embeddings_regularizer=l2(reg),\n",
    "        input_length=1)\n",
    "    # [?, 1, 8]\n",
    "    MF_Embedding_Item = Embedding(\n",
    "        input_dim=n_items,\n",
    "        output_dim=n_emb,\n",
    "        name='item_embedding',\n",
    "        embeddings_initializer='normal',\n",
    "        embeddings_regularizer=l2(reg),\n",
    "        input_length=1)\n",
    "\n",
    "    # Flatten and multiply\n",
    "    # [?, 8]\n",
    "    user_latent = Flatten()(MF_Embedding_User(user))\n",
    "    # [?, 8]\n",
    "    item_latent = Flatten()(MF_Embedding_Item(item))\n",
    "    # [?, 8]\n",
    "    predict_vector = multiply([user_latent, item_latent])\n",
    "\n",
    "    # output layer\n",
    "    prediction = Dense(1, activation='sigmoid',\n",
    "                       kernel_regularizer=l2(reg),\n",
    "                       kernel_initializer='lecun_uniform',\n",
    "                       name='prediction')(predict_vector)\n",
    "\n",
    "    # Model\n",
    "    model = Model(inputs=[user, item], outputs=prediction)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datadir = \"Data_Javier/\"\n",
    "    dataname = \"ml-1m\"\n",
    "    modeldir = \"models\"\n",
    "    n_emb = 8\n",
    "    reg = 0.0\n",
    "    batch_size = 256\n",
    "    epochs = 20\n",
    "    learner = \"adam\"\n",
    "    lr = 0.01\n",
    "    validate_every = 1\n",
    "    save_model = True\n",
    "    topK = 10\n",
    "    n_neg = 4\n",
    "\n",
    "    modelfname = \"keras_GMF\" + \\\n",
    "                 \"_\".join([\"_bs\", str(batch_size)]) + \\\n",
    "                 \"_\".join([\"_reg\", str(reg).replace(\".\", \"\")]) + \\\n",
    "                 \"_\".join([\"_lr\", str(lr).replace(\".\", \"\")]) + \\\n",
    "                 \"_\".join([\"_n_emb\", str(n_emb)]) + \".h5\"\n",
    "    modelpath = os.path.join(modeldir, modelfname)\n",
    "    resultsdfpath = os.path.join(modeldir, 'results_df.p')\n",
    "\n",
    "    dataset = Dataset(os.path.join(datadir, dataname))\n",
    "    train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "    n_users, n_items = train.shape\n",
    "\n",
    "    model = GMF(n_users, n_items, n_emb, reg)\n",
    "    if learner.lower() == \"adagrad\":\n",
    "        model.compile(optimizer=Adagrad(lr=lr), loss='binary_crossentropy')\n",
    "    elif learner.lower() == \"rmsprop\":\n",
    "        model.compile(optimizer=RMSprop(lr=lr), loss='binary_crossentropy')\n",
    "    elif learner.lower() == \"adam\":\n",
    "        model.compile(optimizer=Adam(lr=lr), loss='binary_crossentropy')\n",
    "    else:\n",
    "        model.compile(optimizer=SGD(lr=lr), loss='binary_crossentropy')\n",
    "\n",
    "    best_hr, best_ndcg, best_iter = 0, 0, 0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        t1 = time()\n",
    "        user, item, labels = get_train_instances(train, n_items, n_neg, testNegatives)\n",
    "        hist = model.fit([user, item], labels, batch_size=batch_size, epochs=1, verbose=0, shuffle=True)\n",
    "        t2 = time()\n",
    "        if epoch % validate_every == 0:\n",
    "            (hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK)\n",
    "            hr, ndcg, loss = np.array(hits).mean(), np.array(ndcgs).mean(), hist.history['loss'][0]\n",
    "            print(\"Iteration {}: {:.2f}s, HR = {:.4f}, NDCG = {:.4f}, loss = {:.4f}, validated in {:.2f}s\"\n",
    "                  .format(epoch, t2 - t1, hr, ndcg, loss, time() - t2))\n",
    "            if hr > best_hr:\n",
    "                best_hr, best_ndcg, best_iter, train_time = hr, ndcg, epoch, t2 - t1\n",
    "                if save_model:\n",
    "                    model.save_weights(modelpath, overwrite=True)\n",
    "\n",
    "    print(\"End. Best Iteration {}:  HR = {:.4f}, NDCG = {:.4f}. \"\n",
    "          .format(best_iter, best_hr, best_ndcg))\n",
    "    if save_model:\n",
    "        print(\"The best GMF model is saved to {}\".format(modelpath))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
