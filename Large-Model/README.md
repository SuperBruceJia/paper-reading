# Foundation Model
ChatGPT is not a ConvAI system (chatbot) because it is not trained for multiturn open domain dialog nor is it trained for task oriented dialog.  Using it either as a chatbot or for task completion is bound to disappoint. This class of LLMs with chat interface are distinct from LLM based ConvAI systems such as BlenderBot. There is NO pure generative task oriented dialog systems currently. (Prof. Pascale Fung)

## Course and Tutorials

(1) Stanford CS324 - Advances in Foundation Models

Link: https://stanford-cs324.github.io/winter2022/

Foundation models (FMs) are transforming the landscape of AI in research and industry. Such models (e.g., GPT-3, CLIP, Stable Diffusion) are trained on large amounts of broad data and are adaptable to a wide range of downstream tasks. In this course, students will learn fundamentals behind the models and algorithms, systems and infrastructure, and ethics and societal impacts of foundation models, with an emphasis on gaining hands-on experience and identifying real-world use-cases for FMs. Students will hear from speakers in industry working on foundation models in the wild. The main class assignment will be a quarter-long final project, involving either researching the capabilities of FMs or building an FM-powered application.

---

(2) AACL 2022 Tutorial - Recent Advances in Pre-trained Language Models: Why Do They Work and How to Use Them

Link: https://d223302.github.io/AACL2022-Pretrain-Language-Model-Tutorial/

Pre-trained language models (PLMs) are language models that are pre-trained on large-scaled corpora in a self-supervised fashion. These PLMs have fundamentally changed the natural language processing community in the past few years. In this tutorial, we aim to provide a broad and comprehensive introduction from two perspectives: why those PLMs work, and how to use them in NLP tasks. The first part of the tutorial shows some insightful analysis on PLMs that partially explain their exceptional downstream performance. The second part first focuses on how contrastive learning can be applied on PLMs to improve the representations extracted by the PLMs, and then illustrates how one can apply those PLMs to downstream tasks under different circumstances. These circumstances include fine-tuning PLMs when under data scarcity, and using PLMs with parameter efficiency. We believe that attendees of different backgrounds would find this tutorial informative and useful.

---

(3) Stanford Workshop - Foundation Models

Link: https://crfm.stanford.edu/workshop.html

By foundation model (e.g. BERT, GPT-3, DALL-E), we mean a single model that is trained on raw data, potentially across multiple modalities, which can be usefully adapted to a wide range of tasks. These models have demonstrated clear potential, which we see as the beginnings of a sweeping paradigm shift in AI. They represent a dramatic increase in capability in terms of accuracy, generation quality, and extrapolation to new tasks, but they also pose clear risks such as use for widespread disinformation, potential exacerbation of historical inequities, and problematic centralization of power.

---

(4) NeurIPS 2022 Tutorial - Foundational Robustness of Foundation Models

Link: https://nips.cc/virtual/2022/tutorial/55796 and https://sites.google.com/view/neurips2022-frfm-turotial

Foundation models adopting the methodology of deep learning with pre-training on large-scale unlabeled data and finetuning with task-specific supervision are becoming a mainstream technique in machine learning. Although foundation models hold many promises in learning general representations and few-shot/zero-shot generalization across domains and data modalities, at the same time they raise unprecedented challenges and considerable risks in robustness and privacy due to the use of the excessive volume of data and complex neural network architectures. This tutorial aims to deliver a Coursera-like online tutorial containing comprehensive lectures, a hands-on and interactive Jupyter/Colab live coding demo, and a panel discussion on different aspects of trustworthiness in foundation models.

---

(5) Neural Scaling Laws and Foundation Models

Link: https://sites.google.com/view/nsl-course/schedule

This seminar-style course will focus on recent advances in the rapidly developing area of "foundation models", i.e. large-scale neural network models (e.g., GPT-3, CLIP, DALL-e, etc) pretrained on very large, diverse datasets. Such models often demonstrate significant improvement in their few-shot generalization abilities, as compared to their smaller-scale counterparts, across a wide range of downstream tasks - what one could call a "transformation of quantity into quality" or an "emergent behavior". This is an important step towards a long-standing objective of achieving Artificial General Intelligence (AGI). By AGI here we mean literally a "general", i.e. broad, versatile AI capable of quickly adapting to a wide range of situations and tasks, both novel and those encountered before - i.e. achieving a good stability (memory) vs plasticity (adaptation) trade-off, using the continual learning terminology. In this course, we will survey most recent advances in large-scale pretrained models, focusing specifically on empirical scaling laws of such systems' performance, with increasing compute, model size, and pretraining data (power laws, phase transitions). We will also explore the trade-off between the increasing AI capabilities and AI safety/alignment with human values, considering a range of evaluation metrics beyond the predictive performance. Finally, we will touch upon several related fields, including transfer-, continual- and meta-learning, as well as out-of-distribution generalization, robustness and invariant/causal predictive modeling.

---

(6) Self-supervised Learning

Link: https://github.com/SuperBruceJia/paper-reading/tree/master/Self-supervised%20Learning

---

(7) Computing Architecture for Digital, Al and Quantum Computing

Link: https://github.com/SuperBruceJia/paper-reading/tree/master/Large-Model/Digital-AI-Quantum

---

(8) Stanford MLSys Seminars

Link: https://www.youtube.com/playlist?list=PLSrTvUm384I9PV10koj_cqit9OfbJXEkq

---

(9) Prof. Edward Y. Chang

Link: https://www.youtube.com/@eyuchang

---

(10) Northeastern AI Event

Link: https://ai.northeastern.edu/events/

---

(11) Foundation Models for Decision Making

Link: https://nips.cc/virtual/2022/workshop/49988
