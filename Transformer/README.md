# Connections between Convolution and Self-attention
### Self-attention layers can express any convolutional layers
1. [On the Connection between Local Attention and Dynamic Depth-wise Convolution](https://arxiv.org/abs/2106.04263)
2. [Demystifying Local Vision Transformer: Sparse Connectivity, Weight Sharing, and Dynamic Weight](https://jingdongwang2017.github.io/Pubs/Demystify_Local_Attention.pdf)

<img width="1103" alt="image" src="https://user-images.githubusercontent.com/31528604/203741200-0d839356-a4d7-4da2-b1b5-78669e6b317a.png">

<img width="1084" alt="image" src="https://user-images.githubusercontent.com/31528604/203740765-565a6b22-6b63-4025-9cdc-9a425cf73729.png">

![image](https://user-images.githubusercontent.com/31528604/203011968-ef50e4a9-b2c1-4f6d-b50d-3e7622b4ed9b.png)

![image](https://user-images.githubusercontent.com/31528604/203012818-43cd5df5-34e0-46b7-b450-f93f0c68e951.png)

---

3. [On the Relationship between Self-Attention and Convolutional Layers](https://arxiv.org/abs/1911.03584)

![image](https://user-images.githubusercontent.com/31528604/203018832-cce8a3a4-17ab-4712-8501-d567255faeda.png)
