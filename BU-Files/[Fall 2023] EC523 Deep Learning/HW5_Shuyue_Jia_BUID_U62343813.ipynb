{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l2K7f1ZwG5Z"
      },
      "source": [
        "# HW5: Transformer\n",
        "\n",
        "\n",
        "\n",
        "This assignment will introduce you to\n",
        "\n",
        "1. Understanding the structure of transformer.\n",
        "\n",
        "2. Building a GPT model step by step\n",
        "\n",
        "3. Train a GPT language model to write few sentences.\n",
        "\n",
        "You can run this assignment on Colab or SCC. You are encouraged to use GPU to make training faster, but BE AWARE the Google Colab GPU limit: you are limited to use it for less than 12 hours continuously, after that you may not be able to access it for a particular duration of time unless you purchase Colab pro.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tMPpAqRpR2N"
      },
      "source": [
        "## Q1 Sequence to Sequence Modelling with nn.Transformer (20 points)\n",
        "\n",
        "You will implement a part of transformer. This question aims to let you to get familiar with the transformer architecture purposed in the paper [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf). This question is modified from the original pytorch tutorial [here](https://pytorch.org/tutorials/beginner/transformer_tutorial.html?highlight=transformer), you can refer it when you fill out the code. The general architecture of trasnsformer is shown in the figure below:\n",
        "\n",
        "<img src=\"https://pytorch.org/tutorials/_images/transformer_architecture.jpg\" width=\"360em\">\n",
        "\n",
        "This question requires you to implement a sequence to sequence model by encoder, which is the left part of the figure. You will use integrated layers in pytorch.\n",
        "\n",
        "The transformer model has been proved to be superior in quality for many sequence-to-sequence\n",
        "problems while being more parallelizable. The ``nn.Transformer`` module\n",
        "relies entirely on an attention mechanism (another module recently\n",
        "implemented as `nn.MultiheadAttention`) to draw global dependencies\n",
        "between input and output. The ``nn.Transformer`` module is now highly\n",
        "modularized such that a single component (like [`nn.TransformerEncoder `](<https://pytorch.org/docs/master/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder>)\n",
        "in this tutorial) can be easily adapted/composed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkeaGn8INY9k"
      },
      "source": [
        "### Q1.1 Define the model\n",
        "In this question, we train ``nn.TransformerEncoder`` model on a\n",
        "language modeling task. The language modeling task is to assign a\n",
        "probability for the likelihood of a given word (or a sequence of words)\n",
        "to follow a sequence of words. A sequence of tokens are passed to the embedding\n",
        "layer first, followed by a positional encoding layer to account for the order\n",
        "of the word (see the next paragraph for more details). The\n",
        "``nn.TransformerEncoder`` consists of multiple layers of\n",
        "``nn.TransformerEncoderLayer`` . Along with the input sequence, a square\n",
        "attention mask is required because the self-attention layers in\n",
        "``nn.TransformerEncoder`` are only allowed to attend the earlier positions in\n",
        "the sequence. For the language modeling task, any tokens on the future\n",
        "positions should be masked. To have the actual words, the output\n",
        "of ``nn.TransformerEncoder`` model is sent to the final Linear\n",
        "layer, which is followed by a log-Softmax function. We will see how to implement the ``PositionalEncoding`` in the later question.\n",
        "\n",
        "<img src=\"https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure.ppm\">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHI3LBIcgGVO"
      },
      "source": [
        "In the following model, we only train a encoder model, which is the left part of the figure. Then we concatenate a Linear model `self.decoder` to replace the right part of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ai9dTxjUNS5-"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    '''\n",
        "    This is a transformer encoder model, the input arguments are as follows:\n",
        "    args:\n",
        "    ntoken:  dimension of tokens\n",
        "    ninp: dimension of input embeddings\n",
        "    nhid: dimension of the hidden encoding between two layers of TransformerEncoderLayer\n",
        "    nlayers: number of TransformerEncoderLayer layers\n",
        "    nhead: the number of heads in the multiheadattention model\n",
        "    '''\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout) # PositionalEncoding will be implemented in next section.\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        \"\"\"YOUR CODE HERE\"\"\"\n",
        "        '''\n",
        "        You can use torch.triu and masked_fill to get an upper triangle mask.\n",
        "        The upper right entries are -inf, down left entries including the diagonal are 0.\n",
        "        '''\n",
        "        mask = torch.triu(torch.ones(sz, sz)) == 1\n",
        "        mask = mask.transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "\n",
        "        return mask\n",
        "\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        \"\"\"YOUR CODE HERE\"\"\"\n",
        "        '''\n",
        "        Fill the forward function according to the diagram above.\n",
        "        In the embedding layers, we multiply those weights by square root of\n",
        "        self.ninp.\n",
        "        '''\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask) if src_mask is not None else self.transformer_encoder(src)\n",
        "        output = self.decoder(output)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-BxUnw6MZf5"
      },
      "source": [
        "### Q1.2 Positional Encoding\n",
        "#### Q1.2.1 Fill the code block\n",
        "``PositionalEncoding`` module injects some information about the\n",
        "relative or absolute position of the tokens in the sequence. The\n",
        "positional encodings have the same dimension as the embeddings so that\n",
        "the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
        "different frequencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6A0pUKNMpQ84"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7mD6P7eCaKs"
      },
      "source": [
        "#### Q1.2.2 Why do we need this positional encoding in the transformer architectrue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNI5lKuvCmHu"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ-XugehY_0y"
      },
      "source": [
        "## Q2 Transformer Block for GPT (30 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SucjfHGh_q5v"
      },
      "source": [
        "### Q 2.1 Multi-head self-attention\n",
        "#### Q 2.1.1 The first part is multi-head self-attention. In this layer, you will need to:\n",
        "- Apply linear projections to convert the feature vector at each token into separate vectors for the query, key, and value. The input and output size of linear projection are both `n_embd`\n",
        "- Apply attention, scaling the logits by $\\frac{1}{\\sqrt{d_{qkv}}}$.\n",
        "- Ensure proper masking, such that padding tokens are never attended to.\n",
        "- Perform attention `n_head` times in parallel, where the results are concatenated and then projected using a linear layer.\n",
        "\n",
        "<img src=\"https://www.researchgate.net/publication/332139525/figure/fig3/AS:743081083158528@1554175744311/a-The-Transformer-model-architecture-b-left-Scaled-Dot-Product-Attention.ppm\" width=\"360em\">\n",
        "\n",
        "You should include two types of dropout in your code (with probability set by the  `dropout` argument):\n",
        "- Dropout should be applied to the output of the attention layer (just prior to the residual connection, denoted by \"Add & Norm\" in the first figure)\n",
        "- Dropout should *also* be applied after the final projection.\n",
        "Notes:\n",
        "- Query, key, and value vectors should have shape `[batch_size, n_heads, sequence_len, d_qkv]`\n",
        "- Apply a mask to the scaled dot product of Q and K, before the Softmax function. Let the entry to be a small enough number where the entry of the causal mask is zero. You can use `torch.tril` or `torch.triu` to create a mask, usually we define the mask as a lower triangular matrix. Lower left (incude the diagonal) entries are ones, rest of entries are zeros.\n",
        "Then apply `tensor.masked_fill()` to the output of the scaled dot product of Q and K (It is also the input of softmax). Where the mask is zero, set the input to softmax to a negative number with very large magnitude.\n",
        "- Attention logits and probabilities should have shape `[batch_size, n_heads, sequence_len, sequence_len]`\n",
        "- Vaswani et al. define the output of the attention layer as concatenating the various heads and then multiplying by a matrix $W^O$. It's also possible to implement this is a sum without ever calling `torch.cat`: note that $\\text{Concat}(head_1, \\ldots, head_h)W^O = head_1 W^O_1 + \\ldots + head_h W^O_h$ where $W^O = \\begin{bmatrix} W^O_1\\\\ \\vdots\\\\ W^O_h\\end{bmatrix}$. You may define the `self.proj` this way.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-81DlPewvDRR"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    You can also use torch.nn.MultiheadAttention to validate your implementation\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head, block_size, attn_pdrop=0.1, resid_pdrop=0.1):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0\n",
        "        self.n_head = n_head\n",
        "        #Define key, query, value projections for all heads\n",
        "        self.key = nn.Linear(n_embd, n_embd)\n",
        "        self.query = nn.Linear(n_embd, n_embd)\n",
        "        self.value = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.attn_pdrop = nn.Dropout(attn_pdrop)\n",
        "        self.resid_pdrop = nn.Dropout(resid_pdrop)\n",
        "\n",
        "        mask = torch.tril(torch.ones(block_size, block_size))\n",
        "        self.register_buffer(\"mask\", mask.view(1, 1, block_size, block_size))\n",
        "\n",
        "    def forward(self, x, layer_past=None):\n",
        "        B, T, C = x.size() # B = Batch\n",
        "\n",
        "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        score = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(k.size(-1))\n",
        "\n",
        "        if layer_past is not None:\n",
        "            past_key, past_value = layer_past\n",
        "            k = torch.cat((past_key, k), dim=-2)\n",
        "            v = torch.cat((past_value, v), dim=-2)\n",
        "\n",
        "        if layer_past is None:\n",
        "            score = score.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "\n",
        "        attn = F.softmax(score, dim=-1)\n",
        "        attn = self.attn_pdrop(attn)\n",
        "\n",
        "        context = torch.matmul(attn, v)\n",
        "        context = context.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        output = self.proj(context)\n",
        "        output = self.resid_pdrop(output)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9rHSW_iV8Zl"
      },
      "source": [
        "#### Q 2.1.2 Why do we need to divide a scale of the dot product of Q and K?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g64isWe2WXYW"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKJV23RLAKT3"
      },
      "source": [
        "### Q2.2 Transformer\n",
        "We will implement the transformer block, which is the blue box in the figure. You can use `nn.LayerNorm` layer to apply layer norm. We defined the feed forward layer as `self.mlp`.\n",
        "\n",
        "Notice that where to use the layer norm is a design choice, you can change to see how it affect the final results in the application of Question 3.\n",
        "\n",
        "<img src=\"https://www.researchgate.net/publication/358519951/figure/fig8/AS:1122134894092288@1644549215188/The-GPT-1-architecture-proposed-in-Radford-et-al-a-It-is-composed-of-12-stacked.ppm\" width=\"240em\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "I_L0P2zHAOVt"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\" an Transformer block \"\"\"\n",
        "    def __init__(self, n_embd, n_head, block_size, attn_pdrop=0.1, resid_pdrop=0.1):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.attn = MultiHeadSelfAttention(n_embd, n_head, block_size, attn_pdrop, resid_pdrop)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(resid_pdrop),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygPhx7D_Bsga"
      },
      "source": [
        "## Q3 GPT on Addition (30 points)\n",
        "In this question, we will train an GPT transformer to do addition. We first need to get the dataset and encode addition equation to a vocabulary by integers since we want to use GPT dealing with sequences of integers, and completing them according to patterns in the data.\n",
        "\n",
        "  The sum of two n-digit numbers gives a third up to (n+1)-digit number. So our\n",
        "  encoding will simply be the n-digit first number, n-digit second number, and (n+1)-digit result, all simply concatenated together. Because each addition problem is so structured, there is no need to bother the model with encoding +, =, or other tokens. Each possible sequence has the same length, and simply contains the raw digits of the addition problem. As a few examples, the 2-digit problems:\n",
        "- 85 + 50 = 135 becomes the sequence `[8, 5, 5, 0, 1, 3, 5]`\n",
        "- 6 + 39 = 45 becomes the sequence `[0, 6, 3, 9, 0, 4, 5]`\n",
        "\n",
        "We will also only train GPT on the final (n+1)-digits because the first two n-digits are always assumed to be given. So when we give GPT an exam later, we will e.g. feed it the sequence `[0, 6, 3, 9]`, which encodes that we'd like to add 6 + 39, and hope that the model completes the integer sequence with `[0, 4, 5]` in 3 sequential steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VUGuOjDBGX3H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JwKTM2RgGQ2n"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class AdditionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Define the addition dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ndigit, split):\n",
        "        self.split = split # train/test\n",
        "        self.ndigit = ndigit\n",
        "        self.vocab_size = 10 # 10 possible digits 0..9\n",
        "        # +1 due to potential carry overflow, but then -1 because very last digit doesn't plug back\n",
        "        self.block_size = ndigit + ndigit + ndigit + 1 - 1\n",
        "\n",
        "        # split up all addition problems into either training data or test data\n",
        "        num = (10**self.ndigit)**2 # total number of possible combinations\n",
        "        r = np.random.RandomState(1337) # make deterministic\n",
        "        perm = r.permutation(num)\n",
        "        num_test = min(int(num*0.2), 1000) # 20% of the whole dataset, or only up to 1000\n",
        "        self.ixes = perm[:num_test] if split == 'test' else perm[num_test:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.ixes.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # given a problem index idx, first recover the associated a + b\n",
        "        idx = self.ixes[idx]\n",
        "        nd = 10**self.ndigit\n",
        "        a = idx // nd\n",
        "        b = idx %  nd\n",
        "        c = a + b\n",
        "        render = f'%0{self.ndigit}d%0{self.ndigit}d%0{self.ndigit+1}d' % (a,b,c) # e.g. 03+25=28 becomes \"0325028\"\n",
        "        dix = [int(s) for s in render] # convert each character to its token index\n",
        "        # x will be input to GPT and y will be the associated expected outputs\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long) # predict the next token in the sequence\n",
        "        y[:self.ndigit*2-1] = -100\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lru7Mb4fGV0h",
        "outputId": "ab7b5ff4-8651-4989-f356-7c2f6ec1420d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([4, 7, 1, 7, 0, 6]), tensor([-100, -100, -100,    0,    6,    4]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# create a dataset for e.g. 2-digit addition\n",
        "ndigit = 2\n",
        "train_dataset = AdditionDataset(ndigit=ndigit, split='train')\n",
        "test_dataset = AdditionDataset(ndigit=ndigit, split='test')\n",
        "train_dataset[0] # sample a training instance just to see what one raw example looks like"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uZgtgQlJNEN"
      },
      "source": [
        "### Q3.1 Define the GPT model\n",
        "Now, we start constructing the GPT model. As is shown in the figure, there are 12 transformer blocks concatenated together. In our model, we use `n_layer` to represent the number of blocks. In this question, you need to do the following:\n",
        "\n",
        "- Define the `n_layer` transformer blocks `self.blocks`\n",
        "- Fill out the forward function. Note that the positional embedding is not hard coded as the original transformer, it is learned during training.\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://www.researchgate.net/publication/358519951/figure/fig8/AS:1122134894092288@1644549215188/The-GPT-1-architecture-proposed-in-Radford-et-al-a-It-is-composed-of-12-stacked.ppm\" width=\"240em\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uCozm4EEByR7"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "    \"\"\"  the full GPT language model, with a squence size of block_size \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, n_embd, n_head, block_size, n_layer, embd_pdrop=0.1, attn_pdrop=0.1,resid_pdrop=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # input embedding stem\n",
        "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, block_size, n_embd))\n",
        "        self.drop = nn.Dropout(embd_pdrop)\n",
        "        # transformer\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(n_embd, n_head, block_size, attn_pdrop, resid_pdrop) for _ in range(n_layer)])\n",
        "\n",
        "        # decoder head\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "\n",
        "        self.block_size = block_size\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
        "\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "        \"\"\"\n",
        "        You don't need to change this function. This is setting specific parameters for optimization.\n",
        "        \"\"\"\n",
        "\n",
        "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (torch.nn.Linear, )\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "        for mn, m in self.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
        "\n",
        "                if pn.endswith('bias'):\n",
        "                    # all biases will not be decayed\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "                    # weights of whitelist modules will be weight decayed\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    # weights of blacklist modules will NOT be weight decayed\n",
        "                    no_decay.add(fpn)\n",
        "\n",
        "        # special case the position embedding parameter in the root GPT module as not decayed\n",
        "        no_decay.add('pos_emb')\n",
        "\n",
        "        # validate that we considered every parameter\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
        "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
        "                                                    % (str(param_dict.keys() - union_params), )\n",
        "\n",
        "        # create the pytorch optimizer object\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        b, t = x.size()\n",
        "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
        "\n",
        "        x = self.tok_emb(x)\n",
        "        x = x + self.pos_emb[:, :t, :]\n",
        "        x = self.drop(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        return logits, None\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWEYZY9DF4y7"
      },
      "source": [
        "###Q3.2 Training the model\n",
        "\n",
        "##### You will train the GPT model. Fill out the code of the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ca4alq_1GOM5"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data.dataloader import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA7CPuribP_h"
      },
      "source": [
        "Setting some parameters for training. Initialize the GPT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tsl9tjHrGOvt"
      },
      "outputs": [],
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "class TrainerConfig:\n",
        "    # optimization parameters\n",
        "    max_epochs = 10\n",
        "    batch_size = 64\n",
        "    learning_rate = 3e-4\n",
        "    betas = (0.9, 0.95)\n",
        "    grad_norm_clip = 1.0\n",
        "    weight_decay = 0.1 # only applied on matmul weights\n",
        "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
        "    lr_decay = False\n",
        "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
        "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
        "    # checkpoint settings\n",
        "    ckpt_path = None\n",
        "    num_workers = 0 # for DataLoader\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k,v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "# initialize a baby GPT model\n",
        "model = GPT(vocab_size = train_dataset.vocab_size, n_embd=128, n_head=4, block_size =  train_dataset.block_size, n_layer=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sgav_e5boOD"
      },
      "source": [
        "You need to fill out training process.\n",
        "- Forward the model with current batch `x`, `y`;\n",
        "- Zero the grad before update;\n",
        "- Backward the loss and update the model parameter;\n",
        "- You might want to use `torch.nn.utils.clip_grad_norm_`. The parameter max_norm is `config.grad_norm_clip`;\n",
        "- You will run this getting a loss around 0.1 and accuracy on both train and test around 99%."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.1 Why do we need to use `torch.nn.utils.clip_grad_norm_` in training?"
      ],
      "metadata": {
        "id": "CMflK14DyGX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5QQFRv4I9Vv1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NIhTJaPsM7LD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11c78f5b-e135-4c09-a2a9-3510a147d4f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 1 iter 17: train loss 1.76087. lr 5.994512e-04: 100%|██████████| 18/18 [00:04<00:00,  3.96it/s]\n",
            "epoch 2 iter 17: train loss 1.55655. lr 5.977197e-04: 100%|██████████| 18/18 [00:04<00:00,  3.67it/s]\n",
            "epoch 3 iter 17: train loss 1.36094. lr 5.948114e-04: 100%|██████████| 18/18 [00:05<00:00,  3.49it/s]\n",
            "epoch 4 iter 17: train loss 1.25314. lr 5.907379e-04: 100%|██████████| 18/18 [00:04<00:00,  4.06it/s]\n",
            "epoch 5 iter 17: train loss 1.19641. lr 5.855153e-04: 100%|██████████| 18/18 [00:05<00:00,  3.31it/s]\n",
            "epoch 6 iter 17: train loss 1.12037. lr 5.791641e-04: 100%|██████████| 18/18 [00:05<00:00,  3.31it/s]\n",
            "epoch 7 iter 17: train loss 1.09238. lr 5.717095e-04: 100%|██████████| 18/18 [00:04<00:00,  4.07it/s]\n",
            "epoch 8 iter 17: train loss 1.07742. lr 5.631810e-04: 100%|██████████| 18/18 [00:05<00:00,  3.09it/s]\n",
            "epoch 9 iter 17: train loss 1.03604. lr 5.536122e-04: 100%|██████████| 18/18 [00:04<00:00,  3.97it/s]\n",
            "epoch 10 iter 17: train loss 0.92611. lr 5.430411e-04: 100%|██████████| 18/18 [00:04<00:00,  4.17it/s]\n",
            "epoch 11 iter 17: train loss 0.62494. lr 5.315093e-04: 100%|██████████| 18/18 [00:06<00:00,  2.89it/s]\n",
            "epoch 12 iter 17: train loss 0.55297. lr 5.190624e-04: 100%|██████████| 18/18 [00:04<00:00,  4.09it/s]\n",
            "epoch 13 iter 17: train loss 0.47162. lr 5.057497e-04: 100%|██████████| 18/18 [00:04<00:00,  3.84it/s]\n",
            "epoch 14 iter 17: train loss 0.38767. lr 4.916238e-04: 100%|██████████| 18/18 [00:05<00:00,  3.42it/s]\n",
            "epoch 15 iter 17: train loss 0.36733. lr 4.767405e-04: 100%|██████████| 18/18 [00:04<00:00,  4.04it/s]\n",
            "epoch 16 iter 17: train loss 0.35741. lr 4.611586e-04: 100%|██████████| 18/18 [00:05<00:00,  3.42it/s]\n",
            "epoch 17 iter 17: train loss 0.36289. lr 4.449397e-04: 100%|██████████| 18/18 [00:05<00:00,  3.57it/s]\n",
            "epoch 18 iter 17: train loss 0.36036. lr 4.281479e-04: 100%|██████████| 18/18 [00:04<00:00,  4.04it/s]\n",
            "epoch 19 iter 17: train loss 0.27637. lr 4.108497e-04: 100%|██████████| 18/18 [00:05<00:00,  3.23it/s]\n",
            "epoch 20 iter 17: train loss 0.29650. lr 3.931133e-04: 100%|██████████| 18/18 [00:04<00:00,  3.84it/s]\n",
            "epoch 21 iter 17: train loss 0.19744. lr 3.750088e-04: 100%|██████████| 18/18 [00:04<00:00,  4.04it/s]\n",
            "epoch 22 iter 17: train loss 0.21532. lr 3.566079e-04: 100%|██████████| 18/18 [00:05<00:00,  3.07it/s]\n",
            "epoch 23 iter 17: train loss 0.19262. lr 3.379832e-04: 100%|██████████| 18/18 [00:04<00:00,  4.09it/s]\n",
            "epoch 24 iter 17: train loss 0.17544. lr 3.192084e-04: 100%|██████████| 18/18 [00:04<00:00,  4.03it/s]\n",
            "epoch 25 iter 17: train loss 0.17076. lr 3.003577e-04: 100%|██████████| 18/18 [00:05<00:00,  3.07it/s]\n",
            "epoch 26 iter 17: train loss 0.18078. lr 2.815056e-04: 100%|██████████| 18/18 [00:04<00:00,  4.05it/s]\n",
            "epoch 27 iter 17: train loss 0.16766. lr 2.627266e-04: 100%|██████████| 18/18 [00:04<00:00,  3.95it/s]\n",
            "epoch 28 iter 17: train loss 0.13819. lr 2.440948e-04: 100%|██████████| 18/18 [00:05<00:00,  3.20it/s]\n",
            "epoch 29 iter 17: train loss 0.12157. lr 2.256841e-04: 100%|██████████| 18/18 [00:04<00:00,  4.02it/s]\n",
            "epoch 30 iter 17: train loss 0.11019. lr 2.075671e-04: 100%|██████████| 18/18 [00:05<00:00,  3.22it/s]\n",
            "epoch 31 iter 17: train loss 0.12400. lr 1.898155e-04: 100%|██████████| 18/18 [00:06<00:00,  2.96it/s]\n",
            "epoch 32 iter 17: train loss 0.12657. lr 1.724993e-04: 100%|██████████| 18/18 [00:04<00:00,  4.07it/s]\n",
            "epoch 33 iter 17: train loss 0.09683. lr 1.556871e-04: 100%|██████████| 18/18 [00:05<00:00,  3.52it/s]\n",
            "epoch 34 iter 17: train loss 0.09423. lr 1.394453e-04: 100%|██████████| 18/18 [00:05<00:00,  3.53it/s]\n",
            "epoch 35 iter 17: train loss 0.09139. lr 1.238381e-04: 100%|██████████| 18/18 [00:04<00:00,  4.01it/s]\n",
            "epoch 36 iter 17: train loss 0.10417. lr 1.089272e-04: 100%|██████████| 18/18 [00:05<00:00,  3.21it/s]\n",
            "epoch 37 iter 17: train loss 0.12310. lr 9.477150e-05: 100%|██████████| 18/18 [00:04<00:00,  3.83it/s]\n",
            "epoch 38 iter 17: train loss 0.07785. lr 8.142699e-05: 100%|██████████| 18/18 [00:04<00:00,  4.00it/s]\n",
            "epoch 39 iter 17: train loss 0.07659. lr 6.894639e-05: 100%|██████████| 18/18 [00:05<00:00,  3.03it/s]\n",
            "epoch 40 iter 17: train loss 0.11121. lr 6.000000e-05: 100%|██████████| 18/18 [00:04<00:00,  4.00it/s]\n",
            "epoch 41 iter 17: train loss 0.08505. lr 6.000000e-05: 100%|██████████| 18/18 [00:04<00:00,  4.00it/s]\n",
            "epoch 42 iter 17: train loss 0.09658. lr 6.000000e-05: 100%|██████████| 18/18 [00:05<00:00,  3.06it/s]\n",
            "epoch 43 iter 17: train loss 0.08700. lr 6.000000e-05: 100%|██████████| 18/18 [00:04<00:00,  4.02it/s]\n",
            "epoch 44 iter 17: train loss 0.08203. lr 6.000000e-05: 100%|██████████| 18/18 [00:04<00:00,  3.94it/s]\n",
            "epoch 45 iter 17: train loss 0.08088. lr 6.000000e-05: 100%|██████████| 18/18 [00:06<00:00,  2.66it/s]\n",
            "epoch 46 iter 17: train loss 0.08032. lr 6.000000e-05: 100%|██████████| 18/18 [00:04<00:00,  3.98it/s]\n",
            "epoch 47 iter 17: train loss 0.07339. lr 6.000000e-05: 100%|██████████| 18/18 [00:06<00:00,  2.65it/s]\n",
            "epoch 48 iter 17: train loss 0.08605. lr 6.000000e-05: 100%|██████████| 18/18 [00:04<00:00,  4.06it/s]\n",
            "epoch 49 iter 17: train loss 0.07571. lr 6.000000e-05: 100%|██████████| 18/18 [00:04<00:00,  3.99it/s]\n",
            "epoch 50 iter 17: train loss 0.05732. lr 6.000000e-05: 100%|██████████| 18/18 [00:06<00:00,  2.80it/s]\n"
          ]
        }
      ],
      "source": [
        "config = TrainerConfig(max_epochs=50, batch_size=512, learning_rate=6e-4,\n",
        "                      lr_decay=True, warmup_tokens=1024, final_tokens=50*len(train_dataset)*(ndigit+1),\n",
        "                      num_workers=4)\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.cuda.current_device()\n",
        "  model.to(device)\n",
        "optimizer = model.configure_optimizers(config)\n",
        "\n",
        "\n",
        "\n",
        "tokens = 0\n",
        "for epoch in range(config.max_epochs):\n",
        "    model.train()\n",
        "    data = train_dataset\n",
        "    loader = DataLoader(data, shuffle=True, pin_memory=True,\n",
        "                        batch_size=config.batch_size,\n",
        "                        num_workers=config.num_workers)\n",
        "    losses = []\n",
        "    pbar = tqdm(enumerate(loader), total=len(loader))\n",
        "    for iter, (x, y) in pbar:\n",
        "        # place data on the correct device\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        logits, _ = model(x)\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # decay the learning rate based on our progress\n",
        "        if config.lr_decay:\n",
        "            tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
        "            if tokens < config.warmup_tokens:\n",
        "                # linear warmup\n",
        "                lr_mult = float(tokens) / float(max(1, config.warmup_tokens))\n",
        "            else:\n",
        "                # cosine learning rate decay\n",
        "                progress = float(tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
        "                lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "            lr = config.learning_rate * lr_mult\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr\n",
        "        else:\n",
        "            lr = config.learning_rate\n",
        "        # report progress\n",
        "        pbar.set_description(f\"epoch {epoch+1} iter {iter}: train loss {loss.item():.5f}. lr {lr:e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0gIIlMJiSFC"
      },
      "source": [
        "Now you can run the following code to test the training data sne testing data. You should reach more than 95% correctness on both train and testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-gARh2KE3K5A"
      },
      "outputs": [],
      "source": [
        "def top_k_logits(logits, k):\n",
        "    v, ix = torch.topk(logits, k)\n",
        "    out = logits.clone()\n",
        "    out[out < v[:, [-1]]] = -float('Inf')\n",
        "    return out\n",
        "\n",
        "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None):\n",
        "    \"\"\"\n",
        "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
        "    the sequence, feeding the predictions back into the model each time.\n",
        "    \"\"\"\n",
        "    block_size = train_dataset.block_size\n",
        "    model.eval()\n",
        "    for k in range(steps):\n",
        "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
        "        logits, _ = model(x_cond)\n",
        "        # pluck the logits at the final step and scale by temperature\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        # optionally crop probabilities to only the top k options\n",
        "        if top_k is not None:\n",
        "            logits = top_k_logits(logits, top_k)\n",
        "        # apply softmax to convert to probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # sample from the distribution or take the most likely\n",
        "        if sample:\n",
        "            ix = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "        # append to the sequence and continue\n",
        "        x = torch.cat((x, ix), dim=1)\n",
        "    return x\n",
        "def Addition_GPT(dataset, batch_size=32, max_batches=-1):\n",
        "\n",
        "    results = []\n",
        "    loader = DataLoader(dataset, batch_size=batch_size)\n",
        "    for b, (x, y) in enumerate(loader):\n",
        "        x = x.to(device)\n",
        "        d1d2 = x[:, :ndigit*2]\n",
        "        d1d2d3 = sample(model, d1d2, ndigit+1)\n",
        "        d3 = d1d2d3[:, -(ndigit+1):]\n",
        "        factors = torch.tensor([[10**i for i in range(ndigit+1)][::-1]]).to(device)\n",
        "        # decode the integers from individual digits\n",
        "        d1i = (d1d2[:,:ndigit] * factors[:,1:]).sum(1)\n",
        "        d2i = (d1d2[:,ndigit:ndigit*2] * factors[:,1:]).sum(1)\n",
        "        d3i_pred = (d3 * factors).sum(1)\n",
        "        d3i_gt = d1i + d2i\n",
        "        correct = (d3i_pred == d3i_gt).cpu() # Software 1.0 vs. Software 2.0 fight RIGHT on this line, lol\n",
        "        for i in range(x.size(0)):\n",
        "            results.append(int(correct[i]))\n",
        "            judge = 'CORRECT' if correct[i] else 'WRONG'\n",
        "            if not correct[i]:\n",
        "                print(\"GPT claims that %03d + %03d = %03d (gt is %03d; %s)\"\n",
        "                      % (d1i[i], d2i[i], d3i_pred[i], d3i_gt[i], judge))\n",
        "\n",
        "        if max_batches >= 0 and b+1 >= max_batches:\n",
        "            break\n",
        "\n",
        "    print(\"final score: %d/%d = %.2f%% correct\" % (np.sum(results), len(results), 100*np.mean(results)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pbHtf3tqjJAa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53ae7c2b-7ec0-4ad7-83a4-37bd26f92e50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT claims that 009 + 000 = 019 (gt is 009; WRONG)\n",
            "GPT claims that 045 + 055 = 090 (gt is 100; WRONG)\n",
            "final score: 8998/9000 = 99.98% correct\n"
          ]
        }
      ],
      "source": [
        "Addition_GPT(train_dataset, batch_size=1024, max_batches=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1_sljQGijJ0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "877d3b0f-7be6-44d7-b339-3d911496f8b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT claims that 055 + 045 = 090 (gt is 100; WRONG)\n",
            "final score: 999/1000 = 99.90% correct\n"
          ]
        }
      ],
      "source": [
        "Addition_GPT(test_dataset, batch_size=1024, max_batches=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_wZdbFf842s"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "## Q4 minGPT Text Completion(20 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this question, we will train a GPT to write something fun. First, we need to download some Harry Potter novels as the training dataset. Then a `CharDataset` class is provided to help you prepare training data in the format of characters"
      ],
      "metadata": {
        "id": "INE5JhJN9c_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt\"\n",
        "!wget \"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%202%20-%20The%20Chamber%20Of%20Secrets.txt\"\n",
        "!wget \"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%203%20-%20Prisoner%20of%20Azkaban.txt\"\n"
      ],
      "metadata": {
        "id": "-qy7rPy99cPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d82cac6-614c-43cd-9b59-cfcddbd00967"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-06 22:52:48--  https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 439742 (429K) [text/plain]\n",
            "Saving to: ‘J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt’\n",
            "\n",
            "J. K. Rowling - Har 100%[===================>] 429.44K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-12-06 22:52:49 (11.8 MB/s) - ‘J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt’ saved [439742/439742]\n",
            "\n",
            "--2023-12-06 22:52:49--  https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%202%20-%20The%20Chamber%20Of%20Secrets.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 492130 (481K) [text/plain]\n",
            "Saving to: ‘J. K. Rowling - Harry Potter 2 - The Chamber Of Secrets.txt’\n",
            "\n",
            "J. K. Rowling - Har 100%[===================>] 480.60K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-12-06 22:52:49 (13.1 MB/s) - ‘J. K. Rowling - Harry Potter 2 - The Chamber Of Secrets.txt’ saved [492130/492130]\n",
            "\n",
            "--2023-12-06 22:52:49--  https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%203%20-%20Prisoner%20of%20Azkaban.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 611601 (597K) [text/plain]\n",
            "Saving to: ‘J. K. Rowling - Harry Potter 3 - Prisoner of Azkaban.txt’\n",
            "\n",
            "J. K. Rowling - Har 100%[===================>] 597.27K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-12-06 22:52:49 (13.8 MB/s) - ‘J. K. Rowling - Harry Potter 3 - Prisoner of Azkaban.txt’ saved [611601/611601]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "from collections.abc import Iterable\n",
        "from torch.optim import Optimizer\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "eBlbtywRkH8v"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "klywauhX92uE"
      },
      "outputs": [],
      "source": [
        "class CharDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, block_size):\n",
        "        chars = list(set(data))\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "\n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
        "        self.block_size = block_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.data) / (self.block_size + 1))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # we're actually going to \"cheat\" and pick a spot in the dataset at random\n",
        "        i = np.random.randint(0, len(self.data) - (self.block_size + 1))\n",
        "        chunk = self.data[i:i+self.block_size+1]\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the \"block size\" is the number of characters the model takes as input.\n",
        "# in this case, it can look at up to 128 characters when predicting the next\n",
        "# character.\n",
        "block_size = 128 # spatial extent of the model for its context\n",
        "\n",
        "# For our training set, we will use the text of the first three Harry Potter books.\n",
        "text = open(\"J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt\", 'rb').read()\n",
        "text += open('J. K. Rowling - Harry Potter 2 - The Chamber Of Secrets.txt', 'rb').read()\n",
        "text += open('J. K. Rowling - Harry Potter 3 - Prisoner of Azkaban.txt', 'rb').read()\n",
        "\n",
        "train_dataset = CharDataset(text, block_size)"
      ],
      "metadata": {
        "id": "HEM5HuNGMXMp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f2e3775-2ed1-4497-96cb-8438eeedd86a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 1543473 characters, 87 unique.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpLa2b2V9na5"
      },
      "source": [
        "### Q4.1 Train the model on texts and Save the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are going to train a language model on the `train_dataset`. In this question, you need to finish writing a training loop and save the best model (either the loss is small or the model output is fun) you get. You can refer to the training loop from Q3, but remember to save the model checkpoint(s) while training or at the end of the training loop - You will need it in the next question!\n",
        "\n",
        "\n",
        "\n",
        "In the next cell, an example setup and parameters for the minGPT language model is provided, feel free to change the parameters. If you encounter a training loss plateau - loss is oscillating around some number and the tendency of decreasing is not obvious - and cannot overcome it when trying different parameter combinations,\n",
        "- document the parameters in `TrainerConfig`, training loss curves/loss from the best model you have , the machine you used to train the model and\n",
        "- propose a solution to overcome this plateau  \n",
        "\n",
        "If you there is no such plateau, tell us what are the parameters in the `TrainConfig` and which GPU is used for training.\n",
        "\n",
        "In this question, you are requried to\n",
        "- write a training loop that saves some checkpoints in the training loop\n",
        "- show some discussion on the training loss plateau"
      ],
      "metadata": {
        "id": "LaB_bFtBOgJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = TrainerConfig(max_epochs=200, batch_size=64, learning_rate=6e-4,\n",
        "                      lr_decay=True, warmup_tokens=1024, final_tokens=150*len(train_dataset),\n",
        "                      num_workers=4)\n",
        "\n",
        "model = GPT(vocab_size=train_dataset.vocab_size,n_embd=128, n_head=8,block_size=train_dataset.block_size,n_layer=12,)\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.cuda.current_device()\n",
        "  model.to(device)\n",
        "  optimizer = model.configure_optimizers(config)\n"
      ],
      "metadata": {
        "id": "903fz6PlmuWv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "config = TrainerConfig(\n",
        "    max_epochs=200,\n",
        "    batch_size=64,\n",
        "    learning_rate=6e-4,\n",
        "    lr_decay=True,\n",
        "    warmup_tokens=1024,\n",
        "    final_tokens=150 * len(train_dataset),\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "\n",
        "model = GPT(\n",
        "    vocab_size=train_dataset.vocab_size,\n",
        "    n_embd=128,\n",
        "    n_head=8,\n",
        "    block_size=train_dataset.block_size,\n",
        "    n_layer=12,\n",
        ")\n",
        "\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.cuda.current_device()\n",
        "    model.to(device)\n",
        "    optimizer = model.configure_optimizers(config)\n",
        "\n",
        "\n",
        "best_loss = float('inf')\n",
        "checkpoint_dir = 'checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "for epoch in range(config.max_epochs):\n",
        "    model.train()\n",
        "    data = train_dataset\n",
        "    loader = DataLoader(data, shuffle=True, pin_memory=True,\n",
        "                        batch_size=config.batch_size,\n",
        "                        num_workers=config.num_workers)\n",
        "\n",
        "    losses = []\n",
        "    pbar = tqdm(enumerate(loader), total=len(loader))\n",
        "\n",
        "    for iter, (x, y) in pbar:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        logits, _ = model(x)\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "        losses.append(loss.item())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        if config.lr_decay:\n",
        "            tokens += (y >= 0).sum()\n",
        "            if tokens < config.warmup_tokens:\n",
        "                lr_mult = float(tokens) / float(max(1, config.warmup_tokens))\n",
        "            else:\n",
        "                progress = float(tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
        "                lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "            lr = config.learning_rate * lr_mult\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr\n",
        "\n",
        "        pbar.set_description(f\"epoch {epoch+1} iter {iter}: train loss {loss.item():.5f}. lr {lr:e}\")\n",
        "\n",
        "    avg_loss = sum(losses) / len(losses)\n",
        "\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch + 1}.pt')\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f\"Saved checkpoint at epoch {epoch + 1} with loss {best_loss:.5f}\")\n",
        "\n",
        "\n",
        "final_checkpoint_path = os.path.join(checkpoint_dir, 'final_model.pt')\n",
        "torch.save(model.state_dict(), final_checkpoint_path)\n",
        "print(f\"Training completed. Final model saved at {final_checkpoint_path}\")"
      ],
      "metadata": {
        "id": "m-A9oe3XF82d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "outputId": "88a03f60-bc42-4019-89ba-bbd6368e4e51"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "epoch 1 iter 186: train loss 4.49314. lr 3.979532e-04: 100%|██████████| 187/187 [21:39<00:00,  6.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at epoch 1 with loss 4.49125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "epoch 2 iter 186: train loss 4.49576. lr 3.378866e-04: 100%|██████████| 187/187 [20:05<00:00,  6.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at epoch 2 with loss 4.49115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "epoch 3 iter 186: train loss 4.49595. lr 1.341243e-04: 100%|██████████| 187/187 [19:39<00:00,  6.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at epoch 3 with loss 4.49095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "epoch 4 iter 186: train loss 4.48834. lr 5.594924e-04: 100%|██████████| 187/187 [18:57<00:00,  6.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at epoch 4 with loss 4.49062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "epoch 5 iter 186: train loss 4.49809. lr 6.000000e-05: 100%|██████████| 187/187 [19:02<00:00,  6.11s/it]\n",
            "epoch 6 iter 186: train loss 4.49502. lr 5.771550e-04: 100%|██████████| 187/187 [18:30<00:00,  5.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at epoch 6 with loss 4.49052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "epoch 7 iter 186: train loss 4.48542. lr 1.024592e-04: 100%|██████████| 187/187 [19:31<00:00,  6.26s/it]\n",
            "epoch 8 iter 186: train loss 4.48940. lr 3.769928e-04: 100%|██████████| 187/187 [18:11<00:00,  5.84s/it]\n",
            "epoch 9 iter 186: train loss 4.48803. lr 3.595096e-04: 100%|██████████| 187/187 [18:49<00:00,  6.04s/it]\n",
            "epoch 10 iter 186: train loss 4.49409. lr 1.163195e-04: 100%|██████████| 187/187 [18:58<00:00,  6.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at epoch 10 with loss 4.49050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "epoch 11 iter 5: train loss 4.48339. lr 1.373938e-04:   3%|▎         | 6/187 [00:44<22:09,  7.35s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-7556b762b6a9>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_norm_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion"
      ],
      "metadata": {
        "id": "7QkMyMkYd61t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A situation where the training loss stabilizes without notable improvement, known as a training loss plateau, can be tackled by modifying the learning rate schedule, with the option of more aggressive decay when necessary. Trying out smaller model sizes might result in quicker convergence, especially for simpler tasks such as digit addition. To prevent overfitting, one can implement early stopping by keeping an eye on validation loss. Introducing data augmentation techniques can introduce diversity into the training data, potentially facilitating convergence. Finally, it is essential to engage in hyperparameter tuning, which encompasses batch size, model architecture, and optimizer settings, in order to discover the optimal combination for the given task.\n",
        "\n",
        "\n",
        "It is used to address challenges in the training process of machine learning models. It emphasizes several key aspects:\n",
        "\n",
        "Training Loss Plateau: The issue of a training loss plateau is acknowledged, and the solution involves adjusting the learning rate schedule, with a suggestion for more aggressive decay when necessary. This reflects an understanding of the importance of optimizing the learning process for better model performance.\n",
        "\n",
        "Model Size and Convergence: The paragraph suggests that experimenting with smaller model sizes may lead to faster convergence, especially for simpler tasks like digit addition. This reflects an awareness of the relationship between model complexity and task difficulty.\n",
        "\n",
        "Overfitting Prevention: To prevent overfitting, the paragraph recommends employing early stopping based on monitoring validation loss. This indicates an understanding of the need to balance model complexity and generalization to unseen data.\n",
        "\n",
        "Data Augmentation: The introduction of data augmentation techniques is proposed to diversify the training data, potentially aiding convergence. This shows an appreciation for the impact of data quality and diversity on model training.\n",
        "\n",
        "Hyperparameter Tuning: The paragraph highlights the significance of hyperparameter tuning, including batch size, model architecture, and optimizer settings. This reflects an understanding that the effectiveness of a machine learning model is influenced by various configuration parameters."
      ],
      "metadata": {
        "id": "WAIqnLpqd5Xb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX7m1LVg94Hm"
      },
      "source": [
        "### Q4.2 Load pretrain model and do a prompt writing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the best model from Q4.1, provide a prompt and let the model continue writing for you. Feel free to try different prompts or different models.  \n",
        "If your best model is trained on SCC or the other machines, load the model in the jupyter notebook you are going to submit and print out the 'writing'. Screenshot of the output 'writing' will not be accepted.\n",
        "\n",
        "In this question, you are required to\n",
        "- show the prompt writing output\n",
        "- show some discussion for 4.2.1"
      ],
      "metadata": {
        "id": "aTQ2ThBSA-u3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.1 What would you do to improve the text generation quality(readability, spelling, grammar, logic etc.) of transformer-based language model? If you refer to some papers or posts, remember to cite them."
      ],
      "metadata": {
        "id": "TcDXLTCk_cfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To enhance the quality of generated text, various methods can be utilized. One approach is data augmentation, which involves integrating additional data sources or domains during training and fine-tuning to augment the model's capacity and performance. Another strategy is model optimization, which involves adjusting the architecture and parameters to improve the overall capacity and performance of the model. After text generation, techniques for text refinement are applied to correct errors, including those related to grammar, spelling, punctuation, and style. The use of fine-tuned pretrained language models, achieved through transfer learning, is effective in generating coherent and contextually relevant text. Fine-tuning allows pretrained models to adapt to specific domains or tasks, enhancing the relevance and accuracy of the generated text. Additionally, adversarial training can be employed to improve the language model's robustness during training, enabling it to handle textual manipulations and produce more accurate and contextually appropriate responses. These strategies collectively contribute to the enhancement of text generation quality.\n",
        "\n",
        "Various approaches are used to improve the quality of text generation. These approaches include data augmentation, model optimization, text refinement techniques, leveraging fine-tuned pretrained language models through transfer learning, and employing adversarial training. The goal is to address challenges related to coherence, context, and accuracy in generated text. Each strategy plays a distinct role, collectively contributing to the overall improvement of text generation quality."
      ],
      "metadata": {
        "id": "Yv8_XoP4dOS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example output\n",
        "# model architecture should be defined before you load the parameters\n",
        "PATH='/content/checkpoints/model_epoch_10.pt'\n",
        "checkpoint = torch.load(PATH)\n",
        "model.load_state_dict(checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U67LicYiEs-K",
        "outputId": "3d8c7c48-a854-4c79-a17f-6f093eb4fd05"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example output\n",
        "prompt =\"Harry Potter turned on the TV,  \"\n",
        "context=[ord(c) for c in prompt]\n",
        "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(device)\n",
        "y = sample(model, x, 200, temperature=0.6, sample=True, top_k=5)[0]\n",
        "completion = ''.join([chr(train_dataset.itos[int(i)]) for i in y])\n",
        "print(completion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq9WkmU-FBOs",
        "outputId": "53812765-45b5-4d36-92eb-5f372770a833"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Harry Potter turned on the TV,  IOz\":EE\"1PHShI\"1HShHpEEhHpEz\":EhIOP3I\"HP1hIO\":Ez\"SzHzzwz\"H\"SPzwHPHEH\"SHEz\"H\"SHEzShI\"HS\"H\"\"oIEHzPhIHSHvE\"hooEhHhHSP\":\"n\":Igpgg\"\"vg!\"0IYg!Qh\":ng\"\"noo\"\"Sp!!g!\"hgp!!3gp\"oghoB/oIOOPng\"\"\"\"oo\"\"oo\"\"\"nh\"\"oIOwB\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}