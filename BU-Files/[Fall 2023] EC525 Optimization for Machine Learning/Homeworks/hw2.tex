\documentclass[letterpaper]{article}
\usepackage[utf8]{inputenc}

\usepackage{amsmath,amscd,amssymb,amsthm}
\usepackage[margin=1in]{geometry}

\usepackage{hyperref}

\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathop{\mathbb{E}}}
\newcommand{\argmin}{\mathop{\text{argmin}}}
\newcommand{\argmax}{\mathop{\text{argmax}}}
\renewcommand{\L}{\mathcal{L}}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\op}{\text{op}}
\newcommand{\addspace}{\vskip1.5em}
\newcommand{\solution}{}%\addspace\noindent\textbf{Solution:}\newline}


\title{Optimization for Machine Learning HW 2}

\author{name here
}

\date{Due: 9/20/2023}

\begin{document}

\maketitle
All parts of each question are equally weighted. When solving one question/part, you may assume the results of all previous questions/parts. This HW provides an alternative analysis of SGD in the convex setting that provides a convergence bound for the \emph{last iterate}: $\E[\L(\bw_T)-\L(\bw_\star)]=\tilde O(1/\sqrt{T})$. 
\begin{enumerate}
    \item Prove the following technical identity: for any sequence of numbers $a_1,\dots,a_T$ with $T>1$,
    \begin{align*}
        Ta_T= \sum_{t=1}^T a_t + \sum_{k=1}^{T-1}\frac{T}{(T-k)(T-k+1)}\sum_{t=k}^T (a_t-a_k)
    \end{align*}
     (Hint: There are a number of different ways to show this. One way starts by showing that $\frac{T-k+1}{T-k}\sum_{t=k+1}^T a_t = \sum_{t=k}^T a_t + \frac{1}{T-k}\sum_{t=k}^T (a_t-a_k)$ and uses induction on $k$. Another is to rearrange the terms in the sums to directly show equality. For this, you might want to show the useful identity $\sum_{k=1}^{T-1} b_k \sum_{t=k}^T a_t = \sum_{t=1}^{T-1}a_t \sum_{k=1}^t b_k + a_T \sum_{k=1}^{T-1}b_k$, valid for all $a$ and $b$. You might also want to observe that $\frac{T}{(T-k)(T-k+1)} = \frac{T}{T-k} -\frac{T}{T-k+1}$).
    
    \solution
    
    \addspace
    
    \item Consider stochastic gradient descent with a constant learning rate $\eta$: $\bw_{t+1} = \bw_t - \eta \nabla \ell(\bw_t, z_t)$. Suppose that $\ell$ is convex and $G$-Lipschitz. Show that for all $k$:
    \begin{align*}
        \sum_{t=k}^T \E[\L(\bw_t) - \L(\bw_{k})]\le \frac{\eta (T-k + 1)G^2}{2}
    \end{align*}
    
    \solution
    
    \addspace
    
    \item Show that for for $G$-Lipschitz convex losses, SGD with constant learning rate $\eta = \frac{\|\bw_1-\bw_\star\|}{G\sqrt{T}}$ guarantees:
    \begin{align*}
        \E[\L(\bw_T)-\L(\bw_\star)]&\le O\left(\frac{\|\bw_\star -\bw_1\|G\log(T)}{\sqrt{T}}\right)
    \end{align*}
    (Hint: you will need to show $\sum_{t=1}^T \frac{1}{t} \le 1+\log(T)$. As an intermediate step, try showing $\sum_{t=2}^T \frac{1}{t}\le \int_1^{T} \frac{dt}{t}$ - note the sum starts at $2$. Drawing a picture might help).
    
    By having a learning rate that changes appropriately over time (called a ``schedule'') it is possible to eliminate the logarithmic factor, but it is quite difficult to do so - finding such a schedule was open until as recently as 2019! See \url{https://arxiv.org/abs/1904.12443} for the first such result via a very complicated schedule and analysis. Just this summer, \url{https://arxiv.org/abs/2307.11134} provided a much tighter analysis with a simpler learning rate.
    
    \solution
    
    \addspace
    
    \item[BONUS:] Consider SGD with a \emph{varying} learning rate $\eta_t = \frac{\|\bw_1-\bw_\star\|}{G\sqrt{t}}$. Show that for all $T$:
    \begin{align*}
        \E[\L(\bw_T) - \L(\bw_\star)]&\le O\left(\frac{\|\bw_\star -\bw_1\|G\log(T)}{\sqrt{T}}\right)
    \end{align*}
    
    \solution
    
    \addspace
    
\end{enumerate}

\end{document}