{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** \\_\\_\\_\\_\\_\n",
    "\n",
    "**EID:** \\_\\_\\_\\_\\_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5489 - Tutorial 4\n",
    "## Face Detection in Images\n",
    "\n",
    "In this tutorial you will train a classifier to detect whether there is a face in a small image patch.  This type of face detector is used in your phone and camera whenever you take a picture!\n",
    "\n",
    "First we need to initialize Python.  Run the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import IPython.core.display         \n",
    "# setup output image format (Chrome works best)\n",
    "IPython.core.display.set_matplotlib_formats(\"svg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from numpy import *\n",
    "from sklearn import *\n",
    "import os\n",
    "import zipfile\n",
    "import fnmatch\n",
    "random.seed(100)\n",
    "from scipy import ndimage\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "import skimage.color\n",
    "import skimage.exposure\n",
    "import skimage.io\n",
    "import skimage.util\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Data and Pre-processing\n",
    "Next we need to load the images.  Download `faces.zip`, and put it in the same direcotry as this ipynb file.  **Do not unzip the file.** Then run the following cell to load the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgdata = {'train':[], 'test':[]}\n",
    "classes = {'train':[], 'test':[]}\n",
    "\n",
    "# the dataset is too big, so subsample the training and test sets...\n",
    "# reduce training set by a factor of 4\n",
    "train_subsample = 4  \n",
    "train_counter = [0, 0]\n",
    "# maximum number of samples in each class for test set\n",
    "test_maxsample = 472\n",
    "test_counter = [0, 0]\n",
    "\n",
    "# load the zip file\n",
    "filename = 'faces.zip'\n",
    "zfile = zipfile.ZipFile(filename, 'r')\n",
    "\n",
    "for name in zfile.namelist():\n",
    "    # check file name matches\n",
    "    if fnmatch.fnmatch(name, \"faces/*/*/*.png\"):\n",
    "        \n",
    "        # filename is : faces/train/face/fname.png\n",
    "        (fdir1, fname)  = os.path.split(name)     # get file name\n",
    "        (fdir2, fclass) = os.path.split(fdir1) # get class (face, nonface)\n",
    "        (fdir3, fset)   = os.path.split(fdir2) # get training/test set\n",
    "        # class 1 = face; class 0 = non-face\n",
    "        myclass = int(fclass == \"face\")  \n",
    "\n",
    "        loadme = False\n",
    "        if fset == 'train':\n",
    "            if (train_counter[myclass] % train_subsample) == 0:\n",
    "                loadme = True\n",
    "            train_counter[myclass] += 1\n",
    "        elif fset == 'test':\n",
    "            if test_counter[myclass] < test_maxsample:\n",
    "                loadme = True\n",
    "            test_counter[myclass] += 1\n",
    "            \n",
    "        if (loadme):\n",
    "            # open file in memory, and parse as an image\n",
    "            myfile = zfile.open(name)\n",
    "            #img = matplotlib.image.imread(myfile)\n",
    "            img = skimage.io.imread(myfile)\n",
    "            # convert to grayscale\n",
    "            img = skimage.color.rgb2gray(img)\n",
    "            myfile.close()\n",
    "            \n",
    "            # append data\n",
    "            imgdata[fset].append(img)\n",
    "            classes[fset].append(myclass)\n",
    "\n",
    "        \n",
    "zfile.close()\n",
    "imgsize = img.shape\n",
    "\n",
    "print(len(imgdata['train']))\n",
    "print(len(imgdata['test']))\n",
    "trainclass2start = sum(classes['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is a 19x19 array of pixel values.  Run the below code to show an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.shape)\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(imgdata['train'][0], cmap='gray', interpolation='nearest')\n",
    "plt.title(\"face sample\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(imgdata['train'][trainclass2start], cmap='gray', interpolation='nearest')\n",
    "plt.title(\"non-face sample\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below code to show more images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make an image montage\n",
    "def image_montage(X, imsize=None, maxw=10):\n",
    "    \"\"\"X can be a list of images, or a matrix of vectorized images.\n",
    "      Specify imsize when X is a matrix.\"\"\"\n",
    "    tmp = []\n",
    "    numimgs = len(X)\n",
    "    \n",
    "    # create a list of images (reshape if necessary)\n",
    "    for i in range(0,numimgs):\n",
    "        if imsize != None:\n",
    "            tmp.append(X[i].reshape(imsize))\n",
    "        else:\n",
    "            tmp.append(X[i])\n",
    "    \n",
    "    # add blanks\n",
    "    if (numimgs > maxw) and (mod(numimgs, maxw) > 0):\n",
    "        leftover = maxw - mod(numimgs, maxw)\n",
    "        meanimg = 0.5*(X[0].max()+X[0].min())\n",
    "        for i in range(0,leftover):\n",
    "            tmp.append(ones(tmp[0].shape)*meanimg)\n",
    "    \n",
    "    # make the montage\n",
    "    tmp2 = []\n",
    "    for i in range(0,len(tmp),maxw):\n",
    "        tmp2.append( hstack(tmp[i:i+maxw]) )\n",
    "    montimg = vstack(tmp2) \n",
    "    return montimg\n",
    "\n",
    "# show a few images\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.imshow(image_montage(imgdata['train'][::20]), cmap='gray', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is a 2d array, but the classifier algorithms work on 1d vectors. Run the following code to convert all the images into 1d vectors by flattening.  The result should be a matrix where each row is a flattened image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = empty((len(imgdata['train']), prod(imgsize)))\n",
    "for i,img in enumerate(imgdata['train']):\n",
    "    trainX[i,:] = ravel(img)\n",
    "trainY = asarray(classes['train'])  # convert list to numpy array\n",
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "\n",
    "testX = empty((len(imgdata['test']), prod(imgsize)))\n",
    "for i,img in enumerate(imgdata['test']):\n",
    "    testX[i,:] = ravel(img)\n",
    "testY = asarray(classes['test'])  # convert list to numpy array\n",
    "print(testX.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Detection using  pixel values\n",
    "\n",
    "Train an AdaBoost and GradientBoosting classifiers to classify an image patch as face or non-face.  Also train a kernel SVM classifier using either RBF or polynomial kernel, and a Random Forest Classifier.  Evaluate all your classifiers on the test set.\n",
    "\n",
    "First we will normalize the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler(feature_range=(-1,1))    # make scaling object\n",
    "trainXn = scaler.fit_transform(trainX)   # use training data to fit scaling parameters\n",
    "testXn  = scaler.transform(testX)        # apply scaling to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Which classifier was best?_\n",
    "- **INSERT YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis\n",
    "The accuracy only tells part of the classifier's performance. We can also look at the different types of errors that the classifier makes:\n",
    "- _True Positive (TP)_: classifier correctly said face\n",
    "- _True Negative (TN)_: classifier correctly said non-face\n",
    "- _False Positive (FP)_: classifier said face, but not a face\n",
    "- _False Negative (FN)_: classifier said non-face, but was a face\n",
    "\n",
    "This is summarized in the following table:\n",
    "\n",
    "<table>\n",
    "<tr><th colspan=2 rowspan=2><th colspan=2 style=\"text-align: center\">Actual</th></tr>\n",
    "<tr>  <th>Face</th><th>Non-face</th></tr>\n",
    "<tr><th rowspan=2>Prediction</th><th>Face</th><td>True Positive (TP)</td><td>False Positive (FP)</td></tr>\n",
    "<tr>  <th>Non-face</th><td>False Negative (FN)</td><td>True Negative (TN)</td></tr>\n",
    "</table>\n",
    "\n",
    "We can then look at the _true positive rate_ and the _false positive rate_.\n",
    "- _true positive rate (TPR)_: proportion of true faces that were correctly detected\n",
    "- _false positive rate (FPR)_: proportion of non-faces that were mis-classified as faces.\n",
    "\n",
    "Use the below code to calculate the TPR and FPR of your classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predY is the prediction from the classifier\n",
    "\n",
    "Pind = where(predY==1) # indicies for face predictions\n",
    "Nind = where(predY==0) # indicies for non-face predictions\n",
    "TP = count_nonzero(testY[Pind] == predY[Pind])\n",
    "FP = count_nonzero(testY[Pind] != predY[Pind])\n",
    "TN = count_nonzero(testY[Nind] == predY[Nind])\n",
    "FN = count_nonzero(testY[Nind] != predY[Nind])\n",
    "\n",
    "TPR = TP / (TP+FN)\n",
    "FPR = FP / (FP+TN)\n",
    "\n",
    "print(\"TP=\", TP)\n",
    "print(\"FP=\", FP)\n",
    "print(\"TN=\", TN)\n",
    "print(\"FN=\", FN)\n",
    "print(\"TPR=\", TPR)\n",
    "print(\"FPR=\", FPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_How does the classifier make errors?_\n",
    "- **INSERT YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the AdaBoost classifier, we can interpret what it is doing by looking at which features it uses most in the weak learners.  Use the below code to visualize the pixel features used.\n",
    "\n",
    "Note: if you used GridSearchCV to train the classifier, then you need to use the `best_estimator_` field to access the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaclf is the trained adaboost classifier\n",
    "fi = adaclf.feature_importances_.reshape(imgsize)\n",
    "plt.imshow(fi, interpolation='nearest')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can also look at the important features for xgboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgbclf is the trained xgboost classifier\n",
    "fi = xgbclf.feature_importances_.reshape(imgsize)\n",
    "plt.imshow(fi, interpolation='nearest')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for Random Forests, we can look at the important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# rfclf is the trained random forest classifier\n",
    "fi = rfclf.feature_importances_.reshape(imgsize)\n",
    "plt.imshow(fi, interpolation='nearest')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on which features (pixels) that AdaBoost and Random Forests are using\n",
    "- **INSERT YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For kernel SVM, we can look at the support vectors to see what the classifier finds difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svmclf is the trained SVM classifier\n",
    "\n",
    "print(\"num support vectors:\", len(svmclf.support_vectors_))\n",
    "si  = svmclf.support_  # get indicies of support vectors\n",
    "\n",
    "# get all the patches for each support vector\n",
    "simg = [ imgdata['train'][i] for i in si ]\n",
    "\n",
    "# make montage\n",
    "outimg = image_montage(simg, maxw=20)\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.imshow(outimg, cmap='gray', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on anything you notice about what the SVM finds difficult (i.e., on the decision boundary or within the margin)\n",
    "- **INSERT YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom kernel SVM\n",
    "\n",
    "Now we will try to use a custom kernel with the SVM.  We will consider the following RBF-like kernel based on L1 distance (i.e., cityblock or Manhattan distance),\n",
    "\n",
    "$$ k(\\mathbf{x},\\mathbf{y}) = \\exp \\left(-\\alpha \\sum_{i=1}^d |x_i-y_i|\\right)$$\n",
    "\n",
    "where $x_i,y_i$ are the elements of the vectors $\\mathbf{x},\\mathbf{y}$, and $\\alpha$ is the hyperparameter.  The difference with the RBF kernel is that the new kernel uses the absolute difference rather than the squared difference. Thus, the new kernel does not \"drop off\" as fast as the RBF kernel using squared distance.\n",
    "\n",
    "- Implement the new kernel as a custom kernel function. The `scipy.spatial.distance.cdist` function will be helpful.\n",
    "- Train the SVM with the new kernel.  To select the hyperparameter $\\alpha$, you need to run cross-validation \"manually\" by: 1) trying different values of $\\alpha$, and running cross-validation to select $C$; 2) selecting the  $\\alpha$ with the highest cross-validation score `best_score_` in `GridSearchCV`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does using the new kernel improve the results?\n",
    "- **INSERT YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image Feature Extraction\n",
    "The detection performance is not that good. The problem is that we are using the raw pixel values as features, so it is difficult for the classifier to interpret larger structures of the face that might be important.  To fix the problem, we will extract features from the image using a set of filters.\n",
    "\n",
    "Run the below code to look at the filter output.  The filters are a sets of black and white boxes that respond to similar structures in the image.  After applying the filters to the image, the filter response map is aggregated over a 4x4 window.  Hence each filter produces a 5x5 feature response.  Since there are 4 filters, then the feature vector is 100 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(imgs, doplot=False):\n",
    "    # the filter layout\n",
    "    lay = [array([-1,1]), array([-1,1,-1]),  \n",
    "               array([[1],[-1]]), array([[-1],[1],[-1]])]\n",
    "    sc=8            # size of each filter patch\n",
    "    poolmode = 'i'  # pooling mode (interpolate)\n",
    "    cmode = 'same'  # convolution mode\n",
    "    brick = ones((sc,sc))  # filter patch\n",
    "    ks = []\n",
    "    for l in lay:\n",
    "        tmp = [brick*i for i in l]\n",
    "        if (l.ndim==1):\n",
    "            k = hstack(tmp)\n",
    "        else:\n",
    "            k = vstack(tmp)\n",
    "        ks.append(k)\n",
    "\n",
    "    # get the filter response size\n",
    "    if (poolmode=='max') or (poolmode=='absmax'):\n",
    "        tmpimg = maxpool(maxpool(imgs[0]))\n",
    "    else:\n",
    "        tmpimg = ndimage.interpolation.zoom(imgs[0], 0.25)        \n",
    "    fs = prod(tmpimg.shape)\n",
    "    \n",
    "    # get the total feature length\n",
    "    fst = fs*len(ks)\n",
    "\n",
    "    # filter the images\n",
    "    X  = empty((len(imgs), fst))\n",
    "    for i,img in enumerate(imgs):\n",
    "        x = empty(fst)\n",
    "\n",
    "        # for each filter\n",
    "        for j,th in enumerate(ks):\n",
    "            # filter the image\n",
    "            imgk = signal.convolve(img, ks[j], mode=cmode)\n",
    "            \n",
    "            # do pooling\n",
    "            if poolmode == 'maxabs':\n",
    "                mimg = maxpool(maxpool(abs(imgk)))\n",
    "            elif poolmode == 'max':\n",
    "                mimg = maxpool(maxpool(imgk))\n",
    "            else:\n",
    "                mimg = ndimage.interpolation.zoom(imgk, 0.25)\n",
    "    \n",
    "            # put responses into feature vector\n",
    "            x[(j*fs):(j+1)*fs] = ravel(mimg)\n",
    "               \n",
    "            if (doplot):             \n",
    "                plt.subplot(3,len(ks),j+1)\n",
    "                plt.imshow(ks[j], cmap='gray', interpolation='nearest')\n",
    "                plt.title(\"filter \" + str(j))\n",
    "                plt.subplot(3,len(ks),len(ks)+j+1)\n",
    "                plt.imshow(imgk, cmap='gray', interpolation='nearest')\n",
    "                plt.title(\"filtered image\")\n",
    "                plt.subplot(3,len(ks),2*len(ks)+j+1)\n",
    "                plt.imshow(mimg, cmap='gray', interpolation='nearest')\n",
    "                plt.title(\"image features\")\n",
    "        X[i,:] = x\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new features\n",
    "img = imgdata['train'][0]\n",
    "plt.imshow(img, cmap='gray', interpolation='nearest')\n",
    "plt.title(\"image\")\n",
    "plt.figure(figsize=(9,9))\n",
    "extract_features([img], doplot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets extract image features on the training and test sets.  It may take a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainXf = extract_features(imgdata['train'])\n",
    "print(trainXf.shape)\n",
    "testXf = extract_features(imgdata['test'])\n",
    "print(testXf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection using Image Features\n",
    "Now train AdaBoost and SVM classifiers on the image feature data.  Evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first scale the features\n",
    "scalerf = preprocessing.MinMaxScaler(feature_range=(-1,1))    # make scaling object\n",
    "trainXfn = scalerf.fit_transform(trainXf)   # use training data to fit scaling parameters\n",
    "testXfn  = scalerf.transform(testXf)        # apply scaling to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis\n",
    "Repeat the error analysis for the new classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How has the classifier using image features improved?\n",
    "- **INSERT YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test image\n",
    "Now lets try your face detector on a real image.  Download the \"nasa-small.png\" image and put it in the same directory as your ipynb file.  The below code will load the image, crop out image patches and then extract features. (this may take a few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"nasa-small.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image\n",
    "testimg3 = skimage.io.imread(fname)\n",
    "\n",
    "# convert to grayscale\n",
    "testimg = skimage.color.rgb2gray(testimg3)\n",
    "print(testimg.shape)\n",
    "plt.imshow(testimg, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step size for the sliding window\n",
    "step = 4\n",
    "\n",
    "# extract window patches with step size of 4\n",
    "patches = skimage.util.view_as_windows(testimg, (19,19), step=step)\n",
    "psize = patches.shape\n",
    "# collapse the first 2 dimensions\n",
    "patches2 = patches.reshape((psize[0]*psize[1], psize[2], psize[3]))\n",
    "print(patches2.shape )\n",
    "\n",
    "# histogram equalize patches (improves contrast)\n",
    "patches3 = empty(patches2.shape)\n",
    "for i in range(patches2.shape[0]):\n",
    "    patches3[i,:,:] = skimage.exposure.equalize_hist(patches2[i,:,:])\n",
    "\n",
    "# extract features\n",
    "newXf = extract_features(patches3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now predict using your classifier.  The extracted features are in `newXf`, and scaled features are `newXfn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newXfn  = scalerf.transform(newXf)        # apply scaling to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we we will view the results on the image.  Use the below code. `prednewY` is the vector of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape prediction to an image\n",
    "imgY = prednewY.reshape(psize[0], psize[1])\n",
    "\n",
    "# zoom back to image size\n",
    "imgY2 = ndimage.interpolation.zoom(imgY, step, output=None, order=0)\n",
    "# pad the top and left with half the window size\n",
    "imgY2 = vstack((zeros((9, imgY2.shape[1])), imgY2))\n",
    "imgY2 = hstack((zeros((imgY2.shape[0],9)), imgY2))\n",
    "# pad right and bottom to same size as image\n",
    "if (imgY2.shape[0] != testimg.shape[0]):\n",
    "    imgY2 = vstack((imgY2, zeros((testimg.shape[0]-imgY2.shape[0], imgY2.shape[1]))))\n",
    "if (imgY2.shape[1] != testimg.shape[1]):\n",
    "    imgY2 = hstack((imgY2, zeros((imgY2.shape[0],testimg.shape[1]-imgY2.shape[1]))))\n",
    "    \n",
    "# show detections with image\n",
    "#detimg = dstack(((0.5*imgY2+0.5)*testimg, 0.5*testimg, 0.5*testimg))\n",
    "nimgY2 = 1-imgY2\n",
    "tmp = nimgY2*testimg\n",
    "detimg = dstack((imgY2+tmp, tmp, tmp))\n",
    "\n",
    "# show it!\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.subplot(2,1,1)\n",
    "plt.imshow(imgY2, interpolation='nearest')\n",
    "plt.title('detection map')\n",
    "plt.subplot(2,1,2)\n",
    "plt.imshow(detimg)\n",
    "plt.title('image')\n",
    "plt.axis('image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_How did your face detector do?_\n",
    "- **INSERT YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try it on your own images.  The faces should all be around 19x19 pixels though.\n",
    "We only used 1/4 of the training data. Try using more data to train it!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
