{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** \\_\\_\\_\\_\\_\n",
    "\n",
    "**EID:** \\_\\_\\_\\_\\_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5489 - Tutorial 6\n",
    "## Dimensionality Reduction and Face Recognition\n",
    "\n",
    "In this tutorial you will use linear and non-linear dimensionality reduction on face images, and then train a classifier for face recognition. In the second part, you will then use dimensionality reduction for de-noising.\n",
    "\n",
    "First we need to initialize Python.  Run the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import IPython.core.display         \n",
    "# setup output image format (Chrome works best)\n",
    "IPython.core.display.set_matplotlib_formats(\"svg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from numpy import *\n",
    "from sklearn import *\n",
    "import glob\n",
    "import os\n",
    "random.seed(100)\n",
    "rbow = plt.get_cmap('rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Data and Pre-processing\n",
    "We first need to load the images.  Download `olivetti_py3.pkz` from Canvas, and place it in in the same directory as this ipynb file.  _DO NOT UNZIP IT_.  Then run the following cell to load the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oli = datasets.fetch_olivetti_faces(data_home=\"./\")\n",
    "X = oli.data.astype('float')\n",
    "Y = oli.target\n",
    "img = oli.images\n",
    "imgsize = oli.images[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is a 64x64 array of pixel values, resulting in a 4096 dimensional vector.  Run the below code to show an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(img[0].shape)\n",
    "plt.imshow(img[0], cmap='gray', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below code to show all the images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for i in range(0,400,20):\n",
    "    tmp.append( hstack(img[i:i+20]) )\n",
    "allimg = vstack(tmp)\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.imshow(allimg, cmap='gray', interpolation='nearest')\n",
    "plt.gca().xaxis.set_ticklabels([])\n",
    "plt.gca().yaxis.set_ticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each person is considered as one class, and there are 10 images for each class.  In total there are 40 classes (people).  The data is already vectorized and put into the matrix `X`. The class labels are in vector `Y`. Now we split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly split data into 80% train and 20% test set\n",
    "trainX, testX, trainY, testY = \\\n",
    "  model_selection.train_test_split(X, Y, \n",
    "  train_size=0.80, test_size=0.20, random_state=4487)\n",
    "\n",
    "print(trainX.shape)\n",
    "print(testX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear Dimensionality Reduction - PCA\n",
    "The dimension of the data is too large (4096) so learning classifiers will take a long time.  Instead, our strategy is to use PCA to reduce the dimension first and then use the PCA weights as the representation for each image.  Run PCA oon the data using 9 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function will plot the basis vectors of PCA. Run the next 2 cells to view the PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_basis(model, imgsize):\n",
    "    cname = model.__class__.__name__\n",
    "    if  cname == 'LDA':\n",
    "        KK = model.n_components\n",
    "        comps = model.coef_\n",
    "        mn    = None\n",
    "    elif cname == 'PCA':\n",
    "        KK = model.n_components_\n",
    "        comps = model.components_\n",
    "        mn    = model.mean_\n",
    "    elif cname == 'NMF':\n",
    "        KK = model.n_components_\n",
    "        comps = model.components_\n",
    "        mn    = None\n",
    "    K = KK\n",
    "    if mn is not None:\n",
    "        K += 1        \n",
    "    nr = int(ceil(K/5.0))\n",
    "    sind = 1\n",
    "\n",
    "    #vmin = comps.flatten().min()\n",
    "    #vmax = comps.flatten().max()\n",
    "    \n",
    "    # plot the mean\n",
    "    pcfig = plt.figure(figsize=(8,nr*2))\n",
    "    if mn is not None:\n",
    "        plt.subplot(nr,5,sind)\n",
    "        plt.imshow(mn.reshape(imgsize), interpolation='nearest')\n",
    "        plt.title(\"mean\")\n",
    "        plt.gray()\n",
    "        plt.gca().xaxis.set_ticklabels([])\n",
    "        plt.gca().yaxis.set_ticklabels([])    \n",
    "        sind += 1\n",
    "    # plot the components\n",
    "    for j in range(0,KK):\n",
    "        plt.subplot(nr,5,sind)\n",
    "        v = comps[j,:]            \n",
    "        I = v.reshape(imgsize)\n",
    "        plt.imshow(I, interpolation='nearest')\n",
    "        plt.gray()\n",
    "        plt.title(\"basis \" + str(j+1))\n",
    "        plt.gca().xaxis.set_ticklabels([])\n",
    "        plt.gca().yaxis.set_ticklabels([])\n",
    "        sind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the function\n",
    "plot_basis(pca, imgsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What do the basis images look like? Do some basis images correspond to particular facial features?_\n",
    "- **INSERT YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Recognition\n",
    "Now train a _linear_ classifier to do the face recognition.  Use the calculated PCA representation as the new set of inputs.  Use cross-validation to set the hyperparameters of the classifier.  _You do not need to do cross-validation for the number of components._  Calculate the average training and testing accuracies.  Remember to transform the test data into the PCA representation too!\n",
    "\n",
    "**NOTE**: It is **NOT** recommended to use `svm.SVC` for the training.  This is because the default SVC implements 1-vs-1 multi-class classification, which means it will calculate 40x39=1560 classifiers!  All other classifier should be fine since they do 1-vs-all multi-class classification, e.g., LogisticRegressionCV, LinearSVC, etc.  If you want to use svm.SVC, make sure to use the OneVsRestClassifier as shown in Lecture 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the best number of components\n",
    "Now try a range of number of components for PCA to get the best test accuracy.  Train a classifier for each one and see which dimension gives the best testing accuracy.  Make a plot of PCA dimension vs. test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What is the best number of components?  View the basis images to see what they look like_\n",
    "- **INSERT YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Linear Dimensionality Reduction - NMF\n",
    "Now we will repeat the experiment using non-negative matrix factorization instead of PCA.  Perform NMF with 9 components and visualize the basis images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What do the basis images look like?  What is the main difference with the PCA basis images?_\n",
    "- **INSERT YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the best number of components\n",
    "Now find the number of components that gives the best test accuracy.  Use the same type of classifier that you used in the previous experiment.  Use cross-validation to select the hyperparameters of the classifier.  You do not need to do cross-validation for the number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Which number of components give the best test result? How does the accuracy compare to the best PCA result?  Why is NMF or PCA better?_\n",
    "- **INSERT YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Linear Dimensionality Reduction - KPCA\n",
    "Now we will use KPCA to perform non-linear dimensionality reduction on the data  before training the classifier.  Run KPCA on the data using 10 principal components.  Use the RBF kernel with gamma=0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function will plot the basis vectors of KPCA. Run the next 2 cells to view the PCs.  The kernel PCs are a combination of similarities to points in the training set.  The PCs are visualized by showing the top 5 positive and negative training examples, along with their coefficient $\\alpha_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kbasis(model, imgsize, X):\n",
    "    KK = model.n_components\n",
    "    alphas = model.alphas_.T\n",
    "    minmax = 5\n",
    "    \n",
    "    py = KK\n",
    "    px = minmax*2\n",
    "    for i in range(KK):\n",
    "        # sort alphas\n",
    "        inds = argsort(alphas[i,:])\n",
    "\n",
    "        myi = r_[arange(-1,-minmax-1,-1), arange(minmax-1,-1,-1)]\n",
    "        myinds = inds[myi]\n",
    "        \n",
    "        for j,jj in enumerate(myinds):\n",
    "            plt.subplot(py,px,(j+1)+i*px)\n",
    "            plt.imshow(X[jj,:].reshape(imgsize), interpolation='nearest')\n",
    "            plt.gray()\n",
    "            if alphas[i,jj]<0:\n",
    "                mycol = 'b'\n",
    "            else:\n",
    "                mycol = 'r'\n",
    "            plt.title(\"{:.3f}\".format(alphas[i,jj]), fontsize=7, color=mycol)\n",
    "            if (j==0):\n",
    "                plt.ylabel('PC' + str(i+1))\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the function\n",
    "plt.figure(figsize=(10,12))\n",
    "plot_kbasis(kpca, imgsize, trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What is the interpretation for the KPCA basis?  What kind of faces do some of the PCs prefer?_\n",
    "- **INSERT YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the best kernel and best number of components\n",
    "Now try different kernels (poly, RBF, etc), kernel parameters, and number of components to get the best test accuracy.  Train a classifier for each one and see which dimension gives the best testing accuracy.  Make a plots of number of components vs. test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What is the best kernel and number of components?_\n",
    "- **INSERT YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Which dimensionality reduction method performs the best for classification? Why?*\n",
    "- **INSERT YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De-noising with dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will  use dimensionality reduction for denoising a face image. Given a noisy image $\\mathbf{x}$, we first compute the low-dimensional representation $\\mathbf{w}$, e.g., using PCA. Then given $\\mathbf{w}$, we reconstruct the image $\\hat{\\mathbf{x}}$ from the PCA/NMF basis.  In sklearn, the reconstruction step can be achieved using the `inverse_transform()` method of the PCA or NMF classes.\n",
    "\n",
    "Train a PCA or NMF model using a large number of components (e.g., 100) on the face dataset, and then test it for denoising. For the noisy images, you can try different types of noise, such as:\n",
    "- additive noise (using `random.normal`)\n",
    "- salt & pepper noise (using `random.binomial`)\n",
    "- masking out part of the image (the masked out part can be replaced with black, gray, or white).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for evaluation of the reconstruction or denoising result you can use \"peak signal to noise ratio\", which is implemented in skimage as `skimage.metrics.peak_signal_noise_ratio` or `skimage.measure.compare_psnr` (on older versions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Which kind of noise can be handled well by linear dimensionality reduction? Why?_\n",
    "- **INSERT YOUR ANSWER HERE**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
